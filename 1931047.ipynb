{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 注意\n",
    "\n",
    "本方案完全在官方baseline的基础上进行改动。\n",
    "\n",
    "运行Res_Unimp_Large代码需要使用 **GPU(32G显存)** 来运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. 重要参考\n",
    "### （1）UniMP算法\n",
    "[UniMP算法GitHub链接](https://github.com/PaddlePaddle/PGL/tree/main/ogb_examples/nodeproppred/unimp)\n",
    "\n",
    "**unimp是在ogbn-arixv数据集上刷新了sota的模型，必须马住。**\n",
    "\n",
    "参考论文：\n",
    "* Predict then Propagate: Graph Neural Networks meet Personalized PageRank (https://arxiv.org/abs/1810.05997)\n",
    "* Simple and Deep Graph Convolutional Networks (https://arxiv.org/abs/2007.02133)\n",
    "* Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification (https://arxiv.org/abs/2009.03509)\n",
    "* Combining Label Propagation and Simple Models Out-performs Graph Neural Networks (https://arxiv.org/abs/2010.13993)\n",
    "\n",
    "### （2）图神经网络配套课程\n",
    "[课程链接](https://aistudio.baidu.com/aistudio/education/group/info/1956)\n",
    "\n",
    "老师讲解很细致，理解起来很容易，强烈推荐学习。\n",
    "\n",
    "**课节6中老师专门讲解了这个比赛，并提供了Res修改版的GAT：**\n",
    "\n",
    "[ResGAT](https://aistudio.baidu.com/aistudio/projectdetail/1280598)\n",
    "\n",
    "### （3）参考代码\n",
    "[https://aistudio.baidu.com/aistudio/projectdetail/1467127?channelType=0&channel=0](https://aistudio.baidu.com/aistudio/projectdetail/1467127?channelType=0&channel=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.代码实现\n",
    "\n",
    "### （1）模型构建（mode.py）\n",
    "Res_Unimp_Large代码，也可参见model_modified.py。\n",
    "\n",
    "```\n",
    "class res_unimp_large(object):\n",
    "    def __init__(self, config, num_class):\n",
    "        self.num_class = num_class\n",
    "        self.num_layers = config.get(\"num_layers\", 2)\n",
    "        self.hidden_size = config.get(\"hidden_size\", 128)\n",
    "        self.out_size=config.get(\"out_size\", 40)\n",
    "        self.embed_size=config.get(\"embed_size\", 100)\n",
    "        self.heads = config.get(\"heads\", 8) \n",
    "        self.dropout = config.get(\"dropout\", 0.3)\n",
    "        self.edge_dropout = config.get(\"edge_dropout\", 0.0)\n",
    "        self.use_label_e = config.get(\"use_label_e\", False)\n",
    "    \n",
    "    # 编码输入        \n",
    "    def embed_input(self, feature):   \n",
    "        lay_norm_attr=F.ParamAttr(initializer=F.initializer.ConstantInitializer(value=1))\n",
    "        lay_norm_bias=F.ParamAttr(initializer=F.initializer.ConstantInitializer(value=0))\n",
    "        feature=L.layer_norm(feature, name='layer_norm_feature_input', \n",
    "                                      param_attr=lay_norm_attr, \n",
    "                                      bias_attr=lay_norm_bias)\n",
    "        return feature\n",
    "    \n",
    "    # 连同部分已知的标签编码输入（MaskLabel）\n",
    "    def label_embed_input(self, feature):\n",
    "        label = F.data(name=\"label\", shape=[None, 1], dtype=\"int64\")\n",
    "        label_idx = F.data(name='label_idx', shape=[None, 1], dtype=\"int64\")\n",
    "\n",
    "        label = L.reshape(label, shape=[-1])\n",
    "        label_idx = L.reshape(label_idx, shape=[-1])\n",
    "\n",
    "        embed_attr = F.ParamAttr(initializer=F.initializer.NormalInitializer(loc=0.0, scale=1.0))\n",
    "        embed = F.embedding(input=label, size=(self.out_size, self.embed_size), param_attr=embed_attr )\n",
    "\n",
    "        feature_label = L.gather(feature, label_idx, overwrite=False)\n",
    "        feature_label = feature_label + embed\n",
    "        feature = L.scatter(feature, label_idx, feature_label, overwrite=True)\n",
    "     \n",
    "        lay_norm_attr = F.ParamAttr(initializer=F.initializer.ConstantInitializer(value=1))\n",
    "        lay_norm_bias = F.ParamAttr(initializer=F.initializer.ConstantInitializer(value=0))\n",
    "        feature = L.layer_norm(feature, name='layer_norm_feature_input', \n",
    "                                      param_attr=lay_norm_attr, \n",
    "                                      bias_attr=lay_norm_bias)\n",
    "        return feature\n",
    "        \n",
    "    def forward(self, graph_wrapper, feature, phase):\n",
    "        if phase == \"train\": \n",
    "            edge_dropout = self.edge_dropout\n",
    "            dropout = self.dropout\n",
    "        else:\n",
    "            edge_dropout = 0\n",
    "            dropout = 0\n",
    "\n",
    "        if self.use_label_e:\n",
    "            feature = self.label_embed_input(feature)\n",
    "        else:\n",
    "            feature = self.embed_input(feature)\n",
    "        if dropout > 0:\n",
    "            feature = L.dropout(feature, dropout_prob=dropout, \n",
    "                                    dropout_implementation='upscale_in_train')\n",
    "        \n",
    "        #改变输入特征维度是为了Res连接可以直接相加\n",
    "        feature = L.fc(feature, size=self.hidden_size * self.heads, name=\"init_feature\")\n",
    "\n",
    "\n",
    "        for i in range(self.num_layers - 1):\n",
    "            ngw = pgl.sample.edge_drop(graph_wrapper, edge_dropout) \n",
    "            from model_unimp_large import graph_transformer, attn_appnp\n",
    "\n",
    "            res_feature = feature\n",
    "\n",
    "            feature, _, cks = graph_transformer(str(i), ngw, feature, \n",
    "                                             hidden_size=self.hidden_size,\n",
    "                                             num_heads=self.heads, \n",
    "                                             concat=True, skip_feat=True,\n",
    "                                             layer_norm=True, relu=True, gate=True)\n",
    "            if dropout > 0:\n",
    "                feature = L.dropout(feature, dropout_prob=dropout, \n",
    "                                     dropout_implementation='upscale_in_train') \n",
    "            \n",
    "            # 下面这行便是Res连接了\n",
    "            feature = res_feature + feature \n",
    "        \n",
    "        feature, attn, cks = graph_transformer(str(self.num_layers - 1), ngw, feature, \n",
    "                                             hidden_size=self.out_size,\n",
    "                                             num_heads=self.heads, \n",
    "                                             concat=False, skip_feat=True,\n",
    "                                             layer_norm=False, relu=False, gate=True)\n",
    "\n",
    "        feature = attn_appnp(ngw, feature, attn, alpha=0.2, k_hop=10)\n",
    "\n",
    "        pred = L.fc(\n",
    "            feature, self.num_class, act=None, name=\"pred_output\")\n",
    "        return pred\n",
    "```\n",
    "        \n",
    "### （2）模型配置（Notebook）\n",
    "最优策略：3层res_unimp_large，隐层神经元128个，配置两种dropout，使用MaskLabel，且label_rate = 0.66（在模型训练中设置）。\n",
    "\n",
    "```\n",
    "config = {\n",
    "    \"model_name\": \"res_unimp_large\",\n",
    "    \"num_layers\": 3,\n",
    "    \"hidden_size\": 64,\n",
    "    \"heads\": 2,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"dropout\": 0.33,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"edge_dropout\": 0.32,\n",
    "    \"use_label_e\": True\n",
    "}\n",
    "\n",
    "```\n",
    "### （3）模型训练（Notebook）\n",
    "\n",
    "```\n",
    "import os\n",
    "use_label_e = True\n",
    "label_rate = 0.66\n",
    "epoch = 4000\n",
    "exe.run(startup_program)\n",
    "max_val_acc = 0\n",
    "\n",
    "# 这里可以恢复训练\n",
    "pretrained = False\n",
    "if pretrained:\n",
    "    def name_filter(var):\n",
    "        res = var.name in os.listdir('./output')\n",
    "        return res\n",
    "    fluid.io.load_vars(exe, './output',predicate=name_filter)\n",
    "    max_val_acc = 0.756\n",
    "\n",
    "earlystop = 0\n",
    "# 将图数据变成 feed_dict 用于传入Paddle Excecutor\n",
    "feed_dict = gw.to_feed(dataset.graph)\n",
    "for epoch in range(epoch):\n",
    "    # Full Batch 训练\n",
    "    # 设定图上面那些节点要获取\n",
    "    # node_index: 未知label节点的nid    \n",
    "    # node_label: 未知label\n",
    "    # label_idx: 已知label节点的nid    \n",
    "    # label: 已知label\n",
    "    \n",
    "    if use_label_e:\n",
    "        # 在训练集中抽取部分数据，其Label已知，并可以输入网络训练\n",
    "        train_idx_temp = np.array(train_index, dtype=\"int64\")\n",
    "        train_lab_temp = np.array(train_label, dtype=\"int64\")\n",
    "        state = np.random.get_state()\n",
    "        np.random.shuffle(train_idx_temp)\n",
    "        np.random.set_state(state)\n",
    "        np.random.shuffle(train_lab_temp)\n",
    "\n",
    "        label_idx=train_idx_temp[:int(label_rate*len(train_idx_temp))]\n",
    "        unlabel_idx=train_idx_temp[int(label_rate*len(train_idx_temp)):]\n",
    "        label=train_lab_temp[:int(label_rate*len(train_idx_temp))]\n",
    "        unlabel=train_lab_temp[int(label_rate*len(train_idx_temp)):]\n",
    "\n",
    "        feed_dict[\"node_index\"] = unlabel_idx\n",
    "        feed_dict[\"node_label\"] = unlabel\n",
    "        feed_dict['label_idx']= label_idx\n",
    "        feed_dict['label']= label\n",
    "    else:\n",
    "        feed_dict[\"node_label\"] = np.array(train_label, dtype=\"int64\")\n",
    "        feed_dict[\"node_index\"] = np.array(train_index, dtype=\"int64\")\n",
    "        \n",
    "\n",
    "    train_loss, train_acc = exe.run(train_program,\n",
    "                                feed=feed_dict,\n",
    "                                fetch_list=[loss, acc],\n",
    "                                return_numpy=True)\n",
    "\n",
    "    # Full Batch 验证\n",
    "    # 设定图上面那些节点要获取\n",
    "    # node_index: 未知label节点的nid    \n",
    "    # node_label: 未知label\n",
    "    # label_idx: 已知label节点的nid    \n",
    "    # label: 已知label\n",
    "    \n",
    "    feed_dict[\"node_index\"] = np.array(val_index, dtype=\"int64\")\n",
    "    feed_dict[\"node_label\"] = np.array(val_label, dtype=\"int64\")\n",
    "    if use_label_e:\n",
    "        feed_dict['label_idx'] = np.array(train_index, dtype=\"int64\")\n",
    "        feed_dict['label'] = np.array(train_label, dtype=\"int64\")\n",
    "    val_loss, val_acc = exe.run(test_program,\n",
    "                            feed=feed_dict,\n",
    "                            fetch_list=[v_loss, v_acc],\n",
    "                            return_numpy=True)\n",
    "    print(\"Epoch\", epoch, \"Train Acc\", train_acc[0], \"Valid Acc\", val_acc[0])\n",
    "    \n",
    "    # 保存历史最优验证精度对应的模型\n",
    "    if val_acc[0] > max_val_acc:\n",
    "        max_val_acc = val_acc[0]\n",
    "        fluid.io.save_persistables(exe, './output', train_program)\n",
    "    \n",
    "    # 训练精度持续大于验证精度，结束训练\n",
    "    if train_acc[0] > val_acc[0]:\n",
    "        earlystop += 1\n",
    "        if earlystop == 40:\n",
    "            break\n",
    "    else:\n",
    "        earlystop = 0\n",
    "```\n",
    "\n",
    "### （4）简单投票（本地）\n",
    "```\n",
    "这里将训练出来的文件进行简单投票\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "def vote_merge(filelst):\n",
    "    result = {}\n",
    "    fw = open('D:/subexl/76/merge.csv', encoding='utf-8', mode='w', newline='')\n",
    "    csv_writer = csv.writer(fw)\n",
    "    csv_writer.writerow(['nid', 'label'])\n",
    "    for filepath in filelst:\n",
    "        cr = open(filepath, encoding='utf-8', mode='r')\n",
    "        csv_reader = csv.reader(cr)\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            idx, cls = row\n",
    "            if idx not in result:\n",
    "                result[idx] = []\n",
    "            result[idx].append(cls)\n",
    "\n",
    "    for nid, clss in result.items():\n",
    "        counter = Counter(clss)\n",
    "        true_cls = counter.most_common(1)\n",
    "        csv_writer.writerow([nid, true_cls[0][0]])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    vote_merge([\n",
    " #       \"D:/subexl/75/0.76436.csv\",\n",
    " #       \"D:/subexl/75/0.7635.csv\",\n",
    "  #      \"D:/subexl/75/0.75666.csv\",\n",
    "   #     \"D:/subexl/75/0.75736.csv\",\n",
    " #       \"D:/subexl/75/0.75755.csv\",\n",
    " #       \"D:/subexl/75/0.75801.csv\",\n",
    " #       \"D:/subexl/75/0.75868.csv\",\n",
    " #       \"D:/subexl/75/0.75978.csv\",\n",
    " #       \"D:/subexl/75/0.76171.csv\",\n",
    " #       \"D:/subexl/75/0.76288.csv\",\n",
    " #       \"D:/subexl/75/0.76412.csv\",\n",
    " #       \"D:/subexl/75/0.759664.csv\",\n",
    " #       \"D:/subexl/75/0.75973517.csv\",\n",
    " #       \"D:/subexl/75/0.75980633.csv\",\n",
    " #       \"D:/subexl/75/0.76322347.csv\",\n",
    " #       \"D:/subexl/75/0.763223471.csv\",\n",
    "        \"D:/subexl/76/0.75736.csv\",\n",
    "        \"D:/subexl/76/0.75755.csv\",\n",
    "        \"D:/subexl/76/0.75801.csv\",\n",
    "        \"D:/subexl/76/0.75868.csv\",\n",
    "        \"D:/subexl/76/0.75978.csv\",\n",
    "        \"D:/subexl/76/0.76436.csv\",\n",
    "        \"D:/subexl/76/0.759664.csv\",\n",
    "        \"D:/subexl/76/0.75973517.csv\",\n",
    "        \"D:/subexl/76/0.75980633.csv\",\n",
    "        \"D:/subexl/76/0.76322347.csv\",\n",
    "        \"D:/subexl/76/0.763223471.csv\",\n",
    "        \"D:/subexl/76/submission.csv\",\n",
    "                ])\n",
    "```\n",
    "\n",
    "\n",
    "### （5）绝对多数投票（本地）\n",
    "绝对多数投票法很简单，分类器的投票数超过半数便认可预测结果，否则拒绝。\n",
    "将简单投票后的结果再与新训练出来的文件进行绝对多数投票\n",
    "在这里，我将所有提交文件的名称改为“测试精度.csv”，例如0.76087.csv；然后按照精度大小排序，首先使用绝对多数投票法进行投票，若某一投票不过半数，直接取精度最高csv的预测结果。\n",
    "\n",
    "```\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "#path放的是你所有的提交文件\n",
    "path = 'D:/subexl/75'\n",
    "filelist = os.listdir(path)\n",
    "\n",
    "# 下面这行代码按照测试精度进行排序\n",
    "filelist = sorted(filelist, key= lambda x:float(x[:-4]), reverse=True)\n",
    "print(filelist)\n",
    "\n",
    "# n为测试集条目数\n",
    "n = 37311\n",
    "num_files = len(filelist)\n",
    "a = np.zeros([num_files, n], dtype='i8')\n",
    "for i, filename in enumerate(filelist):\n",
    "    filepath = os.path.join(path, filename)\n",
    "    a[i, :] = np.array(np.loadtxt(filepath, dtype=int, delimiter=',', skiprows=1, usecols=1, encoding='utf-8'))\n",
    "\n",
    "res = np.zeros([n, 1])\n",
    "for j in range(n):\n",
    "    counts = np.bincount(a[:,j])\n",
    "    maxnum = np.argmax(counts)\n",
    "    \n",
    "    # 判读最大投票数是否过半数\n",
    "    if counts[maxnum] > num_files//2:\n",
    "        res[j] = maxnum\n",
    "    else:\n",
    "        res[j] = a[0,j]\n",
    "\n",
    "# 写入文件\n",
    "data=pd.read_csv('D:/subexl/75/0.75897.csv')\n",
    "data['label'] = res\n",
    "data.to_csv('E:/submission.csv',index=False,header=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 总结：\n",
    "主要改进思路嘛是在对预测结果进行简单投票和绝对多数投票下了点功夫，我只用了10个预测样本进行融合，预测的样本更多的话，提交结果的分数可能也越高，具体流程如下图所示。\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/5172690bda594068ac99a25b53427b53e0d5da1f53684fb99d4f184ffecb0cf4)\n",
    "\n",
    "\n",
    "注意图中简单投票和绝对多数投票的样本是任意数目哦，是不是很像人大代表统计各地意见然后在人大提出决议呢，哈哈哈纯属个人想法。\n",
    "最后感谢百度paddlepaddle提供学习的平台。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 以下为原baseline的修改，可以一键运行全部："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 运行方式\n",
    "本次基线基于飞桨PaddlePaddle 1.8.4版本，若本地运行则可能需要额外安装pgl、easydict、pandas等模块。\n",
    "\n",
    "## 本地运行\n",
    "下载左侧文件夹中的所有py文件（包括build_model.py, model.py）,以及work目录，然后在右上角“文件”->“导出Notebook到py”，这样可以保证代码是最新版本），执行导出的py文件即可。完成后下载submission.csv提交结果即可。\n",
    "\n",
    "## AI Studio (Notebook)运行\n",
    "依次运行下方的cell，完成后下载submission.csv提交结果即可。若运行时修改了cell，推荐在右上角重启执行器后再以此运行，避免因内存未清空而产生报错。 Tips：若修改了左侧文件夹中数据，也需要重启执行器后才会加载新文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 代码整体逻辑\n",
    "\n",
    "1. 读取提供的数据集，包含构图以及读取节点特征（用户可自己改动边的构造方式）\n",
    "\n",
    "2. 配置化生成模型，用户也可以根据教程进行图神经网络的实现。\n",
    "\n",
    "3. 开始训练\n",
    "\n",
    "4. 执行预测并产生结果文件\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 环境配置\n",
    "\n",
    "该项目依赖飞桨paddlepaddle==1.8.4, 以及pgl==1.2.0。请按照版本号下载对应版本就可运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cd work && unzip -oq graph.zip  \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting python-dateutil\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 14.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from python-dateutil) (1.15.0)\n",
      "\u001b[31mERROR: blackhole 0.3.2 has requirement xgboost==1.1.0, but you'll have xgboost 1.3.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: python-dateutil\n",
      "  Found existing installation: python-dateutil 2.8.0\n",
      "    Uninstalling python-dateutil-2.8.0:\n",
      "      Successfully uninstalled python-dateutil-2.8.0\n",
      "Successfully installed python-dateutil-2.8.1\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Requirement already satisfied: easydict in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (1.9)\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting pgl==1.2.0\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/35/fa/2290e78914d34d4e4480d7982b8f4d0c58a7e53535113a668a9d75d5c3b6/pgl-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (7.9MB)\n",
      "\u001b[K     |████████████████████████████████| 7.9MB 10.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: visualdl>=2.0.0b; python_version >= \"3\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pgl==1.2.0) (2.1.1)\n",
      "Requirement already satisfied: cython>=0.25.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pgl==1.2.0) (0.29)\n",
      "Requirement already satisfied: numpy>=1.16.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pgl==1.2.0) (1.16.4)\n",
      "Collecting redis-py-cluster (from pgl==1.2.0)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/b2/96/153bbcf5dee29b52b2674e77a87ce864d381f72151737317529b7de4f337/redis_py_cluster-2.1.3-py2.py3-none-any.whl (42kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 21.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (1.1.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (1.15.0)\n",
      "Requirement already satisfied: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (3.8.2)\n",
      "Requirement already satisfied: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (0.7.1.1)\n",
      "Requirement already satisfied: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (3.14.0)\n",
      "Requirement already satisfied: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (0.8.53)\n",
      "Requirement already satisfied: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (1.21.0)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (2.22.0)\n",
      "Requirement already satisfied: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (1.0.0)\n",
      "Requirement already satisfied: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (7.1.2)\n",
      "Collecting redis<4.0.0,>=3.0.0 (from redis-py-cluster->pgl==1.2.0)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/a7/7c/24fb0511df653cf1a5d938d8f5d19802a88cef255706fdda242ff97e91b7/redis-3.5.3-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 14.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (1.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (0.16.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (2.10.1)\n",
      "Requirement already satisfied: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (7.0)\n",
      "Requirement already satisfied: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (2.2.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (0.23)\n",
      "Requirement already satisfied: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (2.6.0)\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (0.6.1)\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (3.9.9)\n",
      "Requirement already satisfied: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (0.18.0)\n",
      "Requirement already satisfied: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (1.4.10)\n",
      "Requirement already satisfied: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (16.7.9)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (1.3.4)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (5.1.2)\n",
      "Requirement already satisfied: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (1.3.0)\n",
      "Requirement already satisfied: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (0.10.0)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (2.0.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (1.25.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (2019.9.11)\n",
      "Requirement already satisfied: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (2.8.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (2019.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (1.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->flake8>=3.7.9->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->flake8>=3.7.9->visualdl>=2.0.0b; python_version >= \"3\"->pgl==1.2.0) (7.2.0)\n",
      "Installing collected packages: redis, redis-py-cluster, pgl\n",
      "Successfully installed pgl-1.2.0 redis-3.5.3 redis-py-cluster-2.1.3\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting pyarrow==0.13.0\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/3f/6c/91a3d949fe0763e60ac181b7b79e74e848e33e402e5e8274cad455519d76/pyarrow-0.13.0-cp37-cp37m-manylinux1_x86_64.whl (48.5MB)\n",
      "\u001b[K     |████████████████████████████████| 48.5MB 8.5MB/s eta 0:00:012\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pyarrow==0.13.0) (1.16.4)\n",
      "Requirement already satisfied: six>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pyarrow==0.13.0) (1.15.0)\n",
      "\u001b[31mERROR: blackhole 0.3.2 has requirement pyarrow>=0.14.1, but you'll have pyarrow 0.13.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: blackhole 0.3.2 has requirement xgboost==1.1.0, but you'll have xgboost 1.3.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pyarrow\n",
      "  Found existing installation: pyarrow 3.0.0\n",
      "    Uninstalling pyarrow-3.0.0:\n",
      "      Successfully uninstalled pyarrow-3.0.0\n",
      "Successfully installed pyarrow-0.13.0\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Requirement already satisfied: chardet==3.0.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "#导入相关包\r\n",
    "!pip install --upgrade python-dateutil\r\n",
    "!pip install easydict\r\n",
    "!pip install pgl==1.2.0 \r\n",
    "!pip install pandas>=0.25\r\n",
    "!pip install pyarrow==0.13.0\r\n",
    "!pip install chardet==3.0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pgl\n",
    "import paddle.fluid as fluid\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 图网络配置\n",
    "\n",
    "这里已经有很多强大的模型配置了，你可以尝试简单的改一下config的字段。\n",
    "例如，换成GAT的配置\n",
    "```\n",
    "config = {\n",
    "    \"model_name\": \"GAT\",\n",
    "    \"num_layers\":  1,\n",
    "    \"dropout\": 0.5,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"edge_dropout\": 0.00,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "config = {\n",
    "    \"model_name\": \"res_unimp_large\",\n",
    "    \"num_layers\": 3,\n",
    "    \"hidden_size\": 64,\n",
    "    \"heads\": 2,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"dropout\": 0.33,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"edge_dropout\": 0.32,\n",
    "    \"use_label_e\": True\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "config = edict(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据加载模块\n",
    "\n",
    "这里主要是用于读取数据集，包括读取图数据构图，以及训练集的划分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Dataset = namedtuple(\"Dataset\", \n",
    "               [\"graph\", \"num_classes\", \"train_index\",\n",
    "                \"train_label\", \"valid_index\", \"valid_label\", \"test_index\"])\n",
    "\n",
    "def load_edges(num_nodes, self_loop=True, add_inverse_edge=True):\n",
    "    # 从数据中读取边\n",
    "    edges = pd.read_csv(\"work/edges.csv\", header=None, names=[\"src\", \"dst\"]).values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if add_inverse_edge:\n",
    "        edges = np.vstack([edges, edges[:, ::-1]])\n",
    "\n",
    "    if self_loop:\n",
    "        src = np.arange(0, num_nodes)\n",
    "        dst = np.arange(0, num_nodes)\n",
    "        self_loop = np.vstack([src, dst]).T\n",
    "        edges = np.vstack([edges, self_loop])\n",
    "    \n",
    "    return edges\n",
    "\n",
    "def load():\n",
    "    # 从数据中读取点特征和边，以及数据划分\n",
    "    node_feat = np.load(\"work/feat.npy\")\n",
    "    num_nodes = node_feat.shape[0]\n",
    "    edges = load_edges(num_nodes=num_nodes, self_loop=True, add_inverse_edge=True)\n",
    "    graph = pgl.graph.Graph(num_nodes=num_nodes, edges=edges, node_feat={\"feat\": node_feat})\n",
    "    \n",
    "    indegree = graph.indegree()\n",
    "    norm = np.maximum(indegree.astype(\"float32\"), 1)\n",
    "    norm = np.power(norm, -0.5)\n",
    "    graph.node_feat[\"norm\"] = np.expand_dims(norm, -1)\n",
    "    \n",
    "    df = pd.read_csv(\"work/train.csv\")\n",
    "    # 打乱顺序\n",
    "    df.sample(frac=1.0) \n",
    "    node_index = df[\"nid\"].values\n",
    "    node_label = df[\"label\"].values\n",
    "    train_part = int(len(node_index) * 0.8)\n",
    "    train_index = node_index[:train_part]\n",
    "    train_label = node_label[:train_part]\n",
    "    valid_index = node_index[train_part:]\n",
    "    valid_label = node_label[train_part:]\n",
    "    test_index = pd.read_csv(\"work/test.csv\")[\"nid\"].values\n",
    "    dataset = Dataset(graph=graph, \n",
    "                    train_label=train_label,\n",
    "                    train_index=train_index,\n",
    "                    valid_index=valid_index,\n",
    "                    valid_label=valid_label,\n",
    "                    test_index=test_index, num_classes=35)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = load()\n",
    "\n",
    "train_index = dataset.train_index\n",
    "train_label = np.reshape(dataset.train_label, [-1 , 1])\n",
    "train_index = np.expand_dims(train_index, -1)\n",
    "\n",
    "val_index = dataset.valid_index\n",
    "val_label = np.reshape(dataset.valid_label, [-1, 1])\n",
    "val_index = np.expand_dims(val_index, -1)\n",
    "\n",
    "test_index = dataset.test_index\n",
    "test_index = np.expand_dims(test_index, -1)\n",
    "test_label = np.zeros((len(test_index), 1), dtype=\"int64\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 组网模块\n",
    "\n",
    "这里是组网模块，目前已经提供了一些预定义的模型，包括**GCN**, **GAT**, **APPNP**等。可以通过简单的配置，设定模型的层数，hidden_size等。你也可以深入到model.py里面，去奇思妙想，写自己的图神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pgl\n",
    "import model\n",
    "import paddle.fluid as fluid\n",
    "import numpy as np\n",
    "import time\n",
    "from build_model import build_model\n",
    "\n",
    "# # 使用CPU\n",
    "#place = fluid.CPUPlace()\n",
    "\n",
    "# 使用GPU\n",
    "place = fluid.CUDAPlace(0)\n",
    "\n",
    "train_program = fluid.default_main_program()\n",
    "startup_program = fluid.default_startup_program()\n",
    "with fluid.program_guard(train_program, startup_program):\n",
    "    with fluid.unique_name.guard():\n",
    "        gw, loss, acc, pred = build_model(dataset,\n",
    "                            config=config,\n",
    "                            phase=\"train\",\n",
    "                            main_prog=train_program)\n",
    "\n",
    "test_program = fluid.Program()\n",
    "with fluid.program_guard(test_program, startup_program):\n",
    "    with fluid.unique_name.guard():\n",
    "        _gw, v_loss, v_acc, v_pred = build_model(dataset,\n",
    "            config=config,\n",
    "            phase=\"test\",\n",
    "            main_prog=test_program)\n",
    "\n",
    "\n",
    "test_program = test_program.clone(for_test=True)\n",
    "\n",
    "exe = fluid.Executor(place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 开始训练过程\n",
    "\n",
    "图神经网络采用FullBatch的训练方式，每一步训练就会把所有整张图训练样本全部训练一遍。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Acc 0.006229062 Valid Acc 0.03367267 train loss 3.9256654 val loss 3.3745832\n",
      "0.03367267\n",
      "Epoch 1 Train Acc 0.03062186 Valid Acc 0.18608956 train loss 3.4400425 val loss 3.0611267\n",
      "0.18608956\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7e55f73769bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m                             \u001b[0mfeed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_acc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                             return_numpy=True)\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Train Acc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Valid Acc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"train loss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"val loss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, program, feed, fetch_list, feed_var_name, fetch_var_name, scope, return_numpy, use_program_cache, return_merged, use_prune)\u001b[0m\n\u001b[1;32m   1064\u001b[0m                 \u001b[0muse_program_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_program_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m                 \u001b[0muse_prune\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_prune\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m                 return_merged=return_merged)\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEOFException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py\u001b[0m in \u001b[0;36m_run_impl\u001b[0;34m(self, program, feed, fetch_list, feed_var_name, fetch_var_name, scope, return_numpy, use_program_cache, return_merged, use_prune)\u001b[0m\n\u001b[1;32m   1152\u001b[0m                 \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m                 \u001b[0mreturn_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_numpy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m                 use_program_cache=use_program_cache)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0mprogram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/executor.py\u001b[0m in \u001b[0;36m_run_program\u001b[0;34m(self, program, feed, fetch_list, feed_var_name, fetch_var_name, scope, return_numpy, use_program_cache)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_program_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m             self._default_executor.run(program.desc, scope, 0, True, True,\n\u001b[0;32m-> 1229\u001b[0;31m                                        fetch_var_name)\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m             self._default_executor.run_prepared_ctx(ctx, scope, False, False,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "use_label_e = True\n",
    "label_rate = 0.66\n",
    "epoch = 500\n",
    "exe.run(startup_program)\n",
    "max_val_acc = 0\n",
    "\n",
    "# 这里可以恢复训练\n",
    "pretrained = False\n",
    "if pretrained:\n",
    "    def name_filter(var):\n",
    "        res = var.name in os.listdir('./output')\n",
    "        return res\n",
    "    fluid.io.load_vars(exe, './output',predicate=name_filter)\n",
    "    max_val_acc = 0.756\n",
    "\n",
    "earlystop = 0\n",
    "# 将图数据变成 feed_dict 用于传入Paddle Excecutor\n",
    "feed_dict = gw.to_feed(dataset.graph)\n",
    "for epoch in range(epoch):\n",
    "    # Full Batch 训练\n",
    "    # 设定图上面那些节点要获取\n",
    "    # node_index: 未知label节点的nid    \n",
    "    # node_label: 未知label\n",
    "    # label_idx: 已知label节点的nid    \n",
    "    # label: 已知label\n",
    "    \n",
    "    if use_label_e:\n",
    "        # 在训练集中抽取部分数据，其Label已知，并可以输入网络训练\n",
    "        train_idx_temp = np.array(train_index, dtype=\"int64\")\n",
    "        train_lab_temp = np.array(train_label, dtype=\"int64\")\n",
    "        state = np.random.get_state()\n",
    "        np.random.shuffle(train_idx_temp)\n",
    "        np.random.set_state(state)\n",
    "        np.random.shuffle(train_lab_temp)\n",
    "\n",
    "        label_idx=train_idx_temp[:int(label_rate*len(train_idx_temp))]\n",
    "        unlabel_idx=train_idx_temp[int(label_rate*len(train_idx_temp)):]\n",
    "        label=train_lab_temp[:int(label_rate*len(train_idx_temp))]\n",
    "        unlabel=train_lab_temp[int(label_rate*len(train_idx_temp)):]\n",
    "\n",
    "        feed_dict[\"node_index\"] = unlabel_idx\n",
    "        feed_dict[\"node_label\"] = unlabel\n",
    "        feed_dict['label_idx']= label_idx\n",
    "        feed_dict['label']= label\n",
    "    else:\n",
    "        feed_dict[\"node_label\"] = np.array(train_label, dtype=\"int64\")\n",
    "        feed_dict[\"node_index\"] = np.array(train_index, dtype=\"int64\")\n",
    "        \n",
    "\n",
    "    train_loss, train_acc = exe.run(train_program,\n",
    "                                feed=feed_dict,\n",
    "                                fetch_list=[loss, acc],\n",
    "                                return_numpy=True)\n",
    "\n",
    "    # Full Batch 验证\n",
    "    # 设定图上面那些节点要获取\n",
    "    # node_index: 未知label节点的nid    \n",
    "    # node_label: 未知label\n",
    "    # label_idx: 已知label节点的nid    \n",
    "    # label: 已知label\n",
    "    \n",
    "    feed_dict[\"node_index\"] = np.array(val_index, dtype=\"int64\")\n",
    "    feed_dict[\"node_label\"] = np.array(val_label, dtype=\"int64\")\n",
    "    if use_label_e:\n",
    "        feed_dict['label_idx'] = np.array(train_index, dtype=\"int64\")\n",
    "        feed_dict['label'] = np.array(train_label, dtype=\"int64\")\n",
    "    val_loss, val_acc = exe.run(test_program,\n",
    "                            feed=feed_dict,\n",
    "                            fetch_list=[v_loss, v_acc],\n",
    "                            return_numpy=True)\n",
    "    print(\"Epoch\", epoch, \"Train Acc\", train_acc[0], \"Valid Acc\", val_acc[0],\"train loss\",train_loss[0],\"val loss\",val_loss[0])\n",
    "    \n",
    "    # 保存历史最优验证精度对应的模型\n",
    "    if val_acc[0] > max_val_acc:\n",
    "        max_val_acc = val_acc[0]\n",
    "        print(val_acc[0])\n",
    "        fluid.io.save_persistables(exe, './output', train_program)\n",
    "    \n",
    "    # 训练精度持续大于验证精度，结束训练\n",
    "    if train_acc[0] > val_acc[0]:\n",
    "        earlystop += 1\n",
    "        if earlystop == 100:\n",
    "            break\n",
    "    else:\n",
    "        earlystop = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 对测试集进行预测\n",
    "\n",
    "训练完成后，我们对测试集进行预测。预测的时候，由于不知道测试集合的标签，我们随意给一些测试label。最终我们获得测试数据的预测结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pretrained = True\r\n",
    "if pretrained:\r\n",
    "    def name_filter(var):\r\n",
    "        res = var.name in os.listdir('./output')\r\n",
    "        return res\r\n",
    "    fluid.io.load_vars(exe, './output',predicate=name_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_dict[\"node_index\"] = np.array(test_index, dtype=\"int64\")\n",
    "feed_dict[\"node_label\"] = np.array(test_label, dtype=\"int64\") #假标签\n",
    "test_prediction = exe.run(test_program,\n",
    "                            feed=feed_dict,\n",
    "                            fetch_list=[v_pred],\n",
    "                            return_numpy=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 生成提交文件\n",
    "\n",
    "最后一步，我们可以使用pandas轻松生成提交文件，最后下载 submission.csv 提交就好了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data={\n",
    "                            \"nid\": test_index.reshape(-1),\n",
    "                            \"label\": test_prediction.reshape(-1)\n",
    "                        })\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def publicnum(num, d = 0):\r\n",
    "    dictnum = {}\r\n",
    "    for i in range(len(num)):\r\n",
    "        if num[i] in dictnum.keys():\r\n",
    "            dictnum[num[i]] += 1\r\n",
    "        else:\r\n",
    "            dictnum.setdefault(num[i], 1)\r\n",
    "    maxnum = 0\r\n",
    "    maxkey = 0\r\n",
    "    for k, v in dictnum.items():\r\n",
    "        if v > maxnum:\r\n",
    "            maxnum = v\r\n",
    "            maxkey = k\r\n",
    "    return maxkey\r\n",
    "\r\n",
    "df0=pd.read_csv(\"submission0.76136.csv\")\r\n",
    "df1=pd.read_csv(\"submission0.757822.csv\")\r\n",
    "df2=pd.read_csv(\"submission0.7583.csv\")\r\n",
    "df3=pd.read_csv(\"submission0.75758.csv\")\r\n",
    "df4=pd.read_csv(\"submission0.75921.csv\")\r\n",
    "df5=pd.read_csv(\"submission0.75782.csv\")\r\n",
    "df6=pd.read_csv(\"submission0.75956.csv\")\r\n",
    "df7=pd.read_csv(\"submission0.75801.csv\")\r\n",
    "df8=pd.read_csv(\"submission0.75884.csv\")\r\n",
    "#df9=pd.read_csv(\"submission9.csv\")\r\n",
    "#df10=pd.read_csv(\"submission10.csv\")\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "nids=[]\r\n",
    "labels=[]\r\n",
    "\r\n",
    "for i in range(df4.shape[0]):\r\n",
    "    label_zs=[]\r\n",
    "    label_zs.append(df0.label[i])\r\n",
    "    label_zs.append(df1.label[i])\r\n",
    "    label_zs.append(df2.label[i])\r\n",
    "    label_zs.append(df3.label[i])\r\n",
    "    label_zs.append(df4.label[i])\r\n",
    "    label_zs.append(df5.label[i])\r\n",
    "    label_zs.append(df6.label[i])\r\n",
    "    label_zs.append(df7.label[i])\r\n",
    "    label_zs.append(df8.label[i])\r\n",
    "    #label_zs.append(df9.label[i])\r\n",
    "    #label_zs.append(df10.label[i])\r\n",
    "    lab=publicnum(label_zs, d = 0)\r\n",
    "    labels.append(lab)\r\n",
    "    nids.append(df4.nid[i])\r\n",
    "\r\n",
    "\r\n",
    "submission = pd.DataFrame(data={\r\n",
    "                            \"nid\": nids,\r\n",
    "                            \"label\": labels\r\n",
    "                        })\r\n",
    "submission.to_csv(\"submissiona.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.8.4 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
